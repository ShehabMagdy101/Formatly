# Turn book into Q/A/Explanation dataset

import os
import json
import config
model = config.LOCAL_MODEL
cooldown_iterations = config.COOLDOWN_ITERATIONS
cooldown_time = config.COOLDOWN_TIME

def generate_qa_explanation(text_chunk: str, is_api=False) -> dict:
    """
    Generates a question, answer, and explanation from given text chunk using API or Local Model 
    
    Args:
        text_chunk (str): The paragraph or chunk of text to be processed
        is_api (bool): decide wether the dataset generated by an API or local model
    
    Returns:
        dict: A dictionary with 'question', 'answer', and 'explanation'
    """
    if is_api:
        qa_data = api_generate(text_chunk)
    else:
        qa_data = local_model_generate(text_chunk)
    return qa_data


def api_generate(text_chunk):
    import google.generativeai as genai

    genai.configure(api_key=os.environ['GEMINI_API_KEY'])
     # instruction prompt
    prompt = config.PROMPT_TEMPLATE.format(text=text_chunk)
    model = genai.GenerativeModel("gemini-1.5-pro")
    response = model.generate_content(prompt)
    output_text = response.text.strip()
    
    try:
        import regex as re
        qa_data = re.sub(r"^```[a-zA-Z]*\n","",output_text, flags=re.MULTILINE)
        qa_data = json.loads(re.sub(r"\n```$", "", qa_data, flags=re.MULTILINE))
    except json.JSONDecodeError:
        qa_data = {"question": None, "answer": None, "explanation": None}
        
    return qa_data


def local_model_generate(text_chunk, model=model):
    from ollama import chat
    from ollama import ChatResponse

    prompt = config.PROMPT_TEMPLATE.format(text=text_chunk)
    try:
        response: ChatResponse = chat(
            model=model,
            messages=[{'role': 'user','content': prompt}],
            options={
                'temperature': 0.7,
                'top_p': 0.9,
                'num_ctx': 1024,  # Reduced context window
                'num_predict': 256,  # Limit response length
                'num_thread': 4,  # Limit CPU threads
            }
        )
        output_text = response['message']['content'] 
        try:
            qa_data = json.loads(output_text)
            return qa_data   
        except json.JSONDecodeError as ex:
            ValueError(f"Invalid JSON output: {ex}") 
    except Exception as e:
       raise RuntimeError(f"Local Model Error: {e}")         

def create_dataset(texts: list[str], filename: str = "dataset.csv"):
    """
    Generate Q/A/Explanation dataset from a list of text chunks and save as CSV.
    
    Args:
        texts (list[str]): List of text chunks to process.
        filename (str): Name of the output CSV file (default 'dataset.csv').
    """
    try:
        
        BASE_DIR = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
        file_path = os.path.join(BASE_DIR, "data", "processed", filename)
        
    except Exception as e:
        return f"Cannot find folder path with error: {(e)}"

    import pandas as pd
    df = pd.DataFrame(columns=["question", "answer", "explanation"])
   
    from tqdm import tqdm
    import time
    flag = True
    iteration_times = []
    data = range(100, 120)
    for i, text in enumerate(tqdm(data, desc="Generating Q&A")):
        try:
            start = time.time()
            json_data = generate_qa_explanation(texts[text])
            row = pd.DataFrame([json_data])
            df = pd.concat([df, row], ignore_index=True)
            end = time.time()
            iteration_times.append(end - start)

            if i == 10:
                avg_iter_time = sum(iteration_times) / len(iteration_times)
                print(f"\nEstimated Time (min): {int(avg_iter_time * len(texts)/60)}")
                choice = input("Do you want to continue ? (Y:N)")
                if choice.lower() == "n":
                    break
                
            if i == cooldown_iterations and config.COOLDOWN == True:
                time.wait(cooldown_time)
        except Exception as e:
            tqdm.write(f"Error: {e}")
            flag = False
            break
    if flag:
        print("Saving dataset...")
        print(f"Dataset saved to: {file_path}")
        df.to_csv(file_path, index=False, encoding="utf-8")
    return df