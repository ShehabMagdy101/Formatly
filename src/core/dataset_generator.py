# Turn pdf into Q/A/Explanation dataset

import os
import json
import config
model = config.LOCAL_MODEL
cooldown_iterations = config.COOLDOWN_ITERATIONS
cooldown_time = config.COOLDOWN_TIME
max_temp = config.MAX_TEMP
temperature = config.TEMPERATURE
top_p = config.TOP_P
num_ctx = config.NUM_CTX
num_predict = config.NUM_PREDICT
num_thread = config.NUM_THREADS
from colorama import Fore
from core.formats import alpaca
from core.formats import chatml
from core.formats import sharegpt


def generate_qa_explanation(text_chunk: str, is_api=False) -> dict:
    """
    Generates a question, answer, and explanation from given text chunk using API or Local Model 
    
    Args:
        text_chunk (str): The paragraph or chunk of text to be processed
        is_api (bool): decide wether the dataset generated by an API or local model
    
    Returns:
        dict: A dictionary with 'question', 'answer', and 'explanation'
    """
    if is_api:
        qa_data = api_generate(text_chunk)
    else:
        qa_data = local_model_generate(text_chunk)
    return qa_data


def api_generate(text_chunk):
    import google.generativeai as genai

    genai.configure(api_key=os.environ['GEMINI_API_KEY'])
     # instruction prompt
    prompt = config.PROMPT_TEMPLATE.format(text=text_chunk)
    model = genai.GenerativeModel("gemini-1.5-pro")
    response = model.generate_content(prompt)
    output_text = response.text.strip()
    
    try:
        import regex as re
        qa_data = re.sub(r"^```[a-zA-Z]*\n","",output_text, flags=re.MULTILINE)
        qa_data = json.loads(re.sub(r"\n```$", "", qa_data, flags=re.MULTILINE))
    except json.JSONDecodeError:
        qa_data = {"question": None, "answer": None, "explanation": None}
        
    return qa_data

def local_model_generate(text_chunk, model=model):
    from ollama import chat
    from ollama import ChatResponse

    prompt = config.PROMPT_TEMPLATE.format(text=text_chunk)
    try:
        response: ChatResponse = chat(
            model=model,
            messages=[{'role': 'user','content': prompt}],
            options={
                'temperature': temperature,
                'top_p':top_p,
                'num_ctx': num_ctx,
                'num_predict': num_predict,  
                'num_thread': num_thread, 
            }
        )
        output_text = response['message']['content'] 
        try:
            qa_data = json.loads(output_text)
            return qa_data   
        except json.JSONDecodeError as ex:
            ValueError(f"Invalid JSON output: {ex}")
            qa_data = {"question": None, "answer": None, "explanation": None} 
    except Exception as e:
       raise RuntimeError(f"Local Model Error: {e}")         


def create_dataset(texts: list[str], format='csv', filename: str = "dataset.csv"):
    """
    Generate Q/A/Explanation dataset from a list of text chunks and save as CSV.
    
    Args:
        texts (list[str]): List of text chunks to process.
        filename (str): Name of the output CSV file (default 'dataset.csv').
    """
    try:
        BASE_DIR = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
        file_path = os.path.join(BASE_DIR, "data", "processed", filename)
        
    except Exception as e:
        return Fore.RED+f"Cannot find folder path with error: {(e)}"

    import pandas as pd
    df = pd.DataFrame(columns=["question", "answer", "explanation"])
   
    from tqdm import tqdm
    import time
    import keyboard
    flag = True
    iteration_times = []
    data = range(0, len(texts))
    pbar = tqdm(data, desc="Generating Q&A") 
    for i, text in enumerate(pbar):
        try:
            if keyboard.is_pressed("q"):  
                print(Fore.YELLOW + "\nDetected 'q' press → Saving and exiting early...")
                flag = True
                break
            start = time.time()
            check_gpu(pbar)
            json_data = generate_qa_explanation(texts[text])
            row = pd.DataFrame([json_data])
            df = pd.concat([df, row], ignore_index=True)
            end = time.time()
            iteration_times.append(end - start)

            if i == 100:
                avg_iter_time = sum(iteration_times) / len(iteration_times)
                print(f"\nEstimated Time (min): {int((avg_iter_time * len(texts) + (len(texts) / cooldown_iterations) * cooldown_time) / 60)}")
                choice = input(Fore.YELLOW+ "Do you want to continue ? (Y:N)")
                if choice.lower() == "n":
                    break
                
            if i % cooldown_iterations == 0 and i != 0 and config.COOLDOWN:
                print(f"System is in Cool Down Mode for {cooldown_time} sec")
                time.sleep(cooldown_time)  
                
        except Exception as e:
            tqdm.write(f"Error: {e}")
            flag = False
            break
    if flag:
        print("Saving dataset...")
        print(Fore.GREEN+ f"CSV Dataset saved to: {file_path}")
        df.dropna(subset=["question"], inplace=True)
        df.to_csv(file_path, index=False, encoding="utf-8")
        if format=='chatml':
            chatml(df)
        elif format=='alpaca':
            alpaca(df)
        elif format=='sharegpt':
            sharegpt(df)
        else:
            pass
    return df


def check_gpu(pbar):
    import sys
    from tqdm import tqdm
    try:
        import pynvml
        pynvml.nvmlInit()
        handle = pynvml.nvmlDeviceGetHandleByIndex(0)
    except Exception as e:
        print(Fore.RED + f"GPU monitoring not available: {e}")
        handle = None

    if handle is None:
        return
    
    temp = pynvml.nvmlDeviceGetTemperature(handle, pynvml.NVML_TEMPERATURE_GPU)
    util = pynvml.nvmlDeviceGetUtilizationRates(handle).gpu
    
    pbar.set_postfix_str(f"GPU Temp: {temp}°C | Util: {util}%")

    if temp >= max_temp:
        tqdm.write(Fore.RED + f"\nCRITICAL: GPU temperature reached {temp}°C. Stopping run!")
        sys.exit(1)


def plot_gpu(gpu, util, iters):
    pass